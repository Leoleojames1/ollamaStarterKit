{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama 101 - A set of basic ollama functions from the ollama pypi docs\n",
    "All functions are referenced from the PyPi ollama package built by Jeffrey Morgan: https://pypi.org/project/ollama/\n",
    "\n",
    "The following notebook is a set of ollama tools and part of an ollama guide made by Leo Borcherding. \n",
    "This notebook is part of the following ollama tutorial https://github.com/Leoleojames1/ollamaStarterKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ollama in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (0.4.7)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from ollama) (2.10.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "# Install Ollama\n",
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama pull/run\n",
    "\n",
    "The following commands will allow you to pull any ollama model, feel free to explore ollama's vast set of models to choose from, here are some of my custom system prompts: https://ollama.com/borch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ollama run phi3\n",
    "\n",
    "&\n",
    "\n",
    "ollama pull phi3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama show & list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ollama show, shows the currently loaded model's modelfile metadata*\n",
    "ollama show --modelfile llama3.2:3b\n",
    "\n",
    "*ollama list shows the available models*\n",
    "ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": \"llama3.2:3b\",\n",
      "  \"modified_at\": \"2024-09-25 17:58:55.324918-05:00\",\n",
      "  \"family\": \"llama\",\n",
      "  \"parameter_size\": \"3.2B\",\n",
      "  \"quantization\": \"Q4_K_M\",\n",
      "  \"parameters\": [\n",
      "    \"stop                           \\\"<|start_header_id|>\\\"\",\n",
      "    \"stop                           \\\"<|end_header_id|>\\\"\",\n",
      "    \"stop                           \\\"<|eot_id|>\\\"\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "MODEL                                    FAMILY     SIZE       QUANT     \n",
      "---------------------------------------------------------------------------\n",
      "hf.co/unsloth/gemma-3-4b-it-GGUF:Q4_K_   gemma3     3.34 GB    unknown   \n",
      "hf.co/Borcherding/Llama-3.2-3B-Finetom   llama      2.02 GB    unknown   \n",
      "hf.co/Borcherding/OARC_Commander_v001_   llama      6.43 GB    unknown   \n",
      "hf.co/Borcherding/OARC_Commander_v001_   llama      3.42 GB    unknown   \n",
      "dolphin3:latest                          llama      4.92 GB    Q4_K_M    \n",
      "marco-o1:latest                          qwen2      4.68 GB    Q4_K_M    \n",
      "qwen2.5-coder:latest                     qwen2      4.68 GB    Q4_K_M    \n",
      "opencoder:latest                         llama      4.74 GB    Q4_K_M    \n",
      "unclemusclez/unsloth-llama3.2:latest     llama      1.92 GB    Q4_0      \n",
      "wojtek/beyonder:4x7b-v3-q4_k             llama      14.61 GB   Q4_K_M    \n",
      "llama2:latest                            llama      3.83 GB    Q4_0      \n",
      "llama3.2:1b-instruct-q2_K                llama      0.58 GB    Q2_K      \n",
      "llama3.2:1b                              llama      1.32 GB    Q8_0      \n",
      "llama3.2:3b                              llama      2.02 GB    Q4_K_M    \n",
      "eas/nous-genstruct:7b-f16                llama      14.48 GB   F16       \n",
      "unclemusclez/smollm-135m-instruct-devi   llama      0.27 GB    F16       \n",
      "borch/Meta-Llama-3.1-8B-Instruct-bnb-4   llama      15.08 GB   Q8_0      \n",
      "CognitiveComputations/dolphin-llama3.1   llama      4.66 GB    Q4_0      \n",
      "stablelm2:1.6b                           stablelm   0.98 GB    Q4_0      \n",
      "smollm:1.7b                              llama      0.99 GB    Q4_0      \n",
      "smollm:360m                              llama      0.23 GB    Q4_0      \n",
      "smollm:135m                              llama      0.09 GB    Q4_0      \n",
      "mannix/llama3.1-8b-abliterated:latest    llama      4.66 GB    Q4_0      \n",
      "sebdg/emotional_llama:latest             llama      4.92 GB    Q4_K_M    \n",
      "mathstral:latest                         llama      4.11 GB    Q4_0      \n",
      "llama3.1:8b                              llama      4.66 GB    Q4_0      \n",
      "qnguyen3/nanollava:latest                qwen2      2.11 GB    F16       \n",
      "CognitiveComputations/dolphin-2.9.2-qw   qwen2      3.02 GB    Q2_K      \n",
      "qwen2:latest                             qwen2      4.43 GB    Q4_0      \n",
      "borch/emotional_llama_speed_chat:lates   llama      4.92 GB    Q4_K_M    \n",
      "nomic-embed-text:latest                  nomic-be   0.27 GB    F16       \n",
      "borch/phi3_speed_chat:latest             llama      2.32 GB    Q4_K_M    \n",
      "phi3:latest                              llama      2.32 GB    Q4_K_M    \n",
      "borch/gpt2-chatbot-q8_0:latest           gpt2       1.75 GB    Q8_0      \n",
      "sunapi386/llama-3-lexi-uncensored:8b     llama      4.92 GB    Q4_K_M    \n",
      "mistral:latest                           llama      4.11 GB    Q4_0      \n",
      "llava:latest                             llama      4.73 GB    Q4_0      \n",
      "llava-phi3:latest                        llama      2.93 GB    Q4_K_M    \n",
      "llama3:latest                            llama      4.66 GB    Q4_0      \n",
      "dolphin-phi:latest                       phi2       1.60 GB    Q4_0      \n",
      "dolphin-mistral:latest                   llama      4.11 GB    Q4_0      \n",
      "dolphin-llama3:latest                    llama      4.66 GB    Q4_0      \n",
      "borch/llama3_speed_chat_2:latest         llama      4.66 GB    Q4_0      \n",
      "borch/llava-llama-3-8b-Q3_K_M:latest     llama      4.02 GB    Q3_K_M    \n",
      "borch/llava-phi-3-mini-int4:latest       llama      2.32 GB    Q4_K_M    \n",
      "r2-d2:latest                             llama      4.66 GB    Q4_0      \n",
      "RicknMorty:latest                        llama      4.66 GB    Q4_0      \n",
      "Dolphin_Eminem_Rap_God:latest            llama      4.11 GB    Q4_0      \n",
      "borch_llama3po:latest                    llama      4.66 GB    Q4_0      \n",
      "borch_llama3_speed_chat_shrek:latest     llama      4.66 GB    Q4_0      \n",
      "borch_llama3_speed_chat:latest           llama      4.66 GB    Q4_0      \n",
      "Jesus:latest                             llama      4.66 GB    Q4_0      \n",
      "C3PO:latest                              llama      4.66 GB    Q4_0      \n"
     ]
    }
   ],
   "source": [
    "# Import the Ollama library\n",
    "import ollama\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Show information about a specific model\n",
    "model_info = ollama.show('llama3.2:3b')\n",
    "\n",
    "# Convert to a dictionary for easier formatting\n",
    "model_dict = {\n",
    "    \"model\": \"llama3.2:3b\",\n",
    "    \"modified_at\": str(model_info.modified_at),\n",
    "    \"family\": model_info.details.family,\n",
    "    \"parameter_size\": model_info.details.parameter_size,\n",
    "    \"quantization\": model_info.details.quantization_level,\n",
    "    \"parameters\": model_info.parameters.split('\\n')\n",
    "}\n",
    "\n",
    "# Pretty print the model information\n",
    "print(json.dumps(model_dict, indent=2))\n",
    "\n",
    "# List all available models\n",
    "models = ollama.list()\n",
    "\n",
    "# Print a simplified table of models\n",
    "print(\"\\n{:<40} {:<10} {:<10} {:<10}\".format(\"MODEL\", \"FAMILY\", \"SIZE\", \"QUANT\"))\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for model in models.models:\n",
    "    size_gb = f\"{model.size / 1_000_000_000:.2f} GB\"\n",
    "    print(\"{:<40} {:<10} {:<10} {:<10}\".format(\n",
    "        model.model[:38], \n",
    "        model.details.family[:8], \n",
    "        size_gb, \n",
    "        model.details.quantization_level[:8]\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ChatResponse method can be used for direct back and forth messaging with your ollama model, but will not provide access to prompt streaming. For Streaming follow along to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\" \n",
      "\n",
      "I was trained on a massive dataset of text from the internet and other sources, which I use to generate human-like responses to a wide range of questions and prompts. This training data includes but is not limited to books, articles, research papers, conversations, and more.\n",
      "\n",
      "My primary function is to process natural language inputs and produce coherent, contextually relevant, and accurate text outputs. \n",
      "\n",
      "I was developed by Meta AI, a company that focuses on developing and applying various forms of artificial intelligence to help humans learn, communicate, and solve complex problems.\n",
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\" \n",
      "\n",
      "I was trained on a massive dataset of text from the internet and other sources, which I use to generate human-like responses to a wide range of questions and prompts. This training data includes but is not limited to books, articles, research papers, conversations, and more.\n",
      "\n",
      "My primary function is to process natural language inputs and produce coherent, contextually relevant, and accurate text outputs. \n",
      "\n",
      "I was developed by Meta AI, a company that focuses on developing and applying various forms of artificial intelligence to help humans learn, communicate, and solve complex problems.\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='llama3.2:3b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Who are you? And what were you trained on?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a nice interactive chat interface for your Ollama models that works directly in the notebook. This interface includes:\n",
    "\n",
    "1. A dropdown to select the ollama model\n",
    "2. A text input for messages\n",
    "3. A scrollable chat history display\n",
    "4. A reset button to start fresh conversations\n",
    "5. Streaming responses that appear character by character\n",
    "\n",
    "To use it, simply run this cell and you'll get a fully functional chat interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Chat initialized with model: llama3.2:3b\n",
      "Type 'exit' to end the chat, 'clear' to reset the conversation, or 'change model:[name]' to switch models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ AI: It looks like you're testing to see if I'm working properly. Is there something I can help you with or would you like to have a conversation?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 110\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Create and start the chat interface\u001b[39;00m\n\u001b[0;32m    109\u001b[0m chat \u001b[38;5;241m=\u001b[39m SimpleOllamaChat()\n\u001b[1;32m--> 110\u001b[0m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m, in \u001b[0;36mSimpleOllamaChat.start_chat\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Start the interactive chat loop.\"\"\"\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Get user input\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     user_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43müë§ You: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Process commands\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_message\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ADA\\miniconda3\\envs\\imgAnno2\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADA\\miniconda3\\envs\\imgAnno2\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class SimpleOllamaChat:\n",
    "    def __init__(self, model=\"llama3.2:3b\"):\n",
    "        \"\"\"Initialize the chat interface with a specified model.\"\"\"\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "        self.running = True\n",
    "        print(f\"ü§ñ Chat initialized with model: {self.model}\")\n",
    "        print(\"Type 'exit' to end the chat, 'clear' to reset the conversation, or 'change model:[name]' to switch models.\")\n",
    "        \n",
    "    def display_models(self):\n",
    "        \"\"\"Display available models in a clean format.\"\"\"\n",
    "        try:\n",
    "            models_list = ollama.list()\n",
    "            print(\"\\nAvailable models:\")\n",
    "            print(\"-\" * 60)\n",
    "            for model in models_list.models[:10]:  # Show only first 10 models to avoid clutter\n",
    "                print(f\"‚Ä¢ {model.model}\")\n",
    "            if len(models_list.models) > 10:\n",
    "                print(f\"... and {len(models_list.models) - 10} more models\")\n",
    "            print(\"-\" * 60)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error fetching models: {e}\")\n",
    "    \n",
    "    def start_chat(self):\n",
    "        \"\"\"Start the interactive chat loop.\"\"\"\n",
    "        while self.running:\n",
    "            # Get user input\n",
    "            user_message = input(\"\\nüë§ You: \")\n",
    "            \n",
    "            # Process commands\n",
    "            if user_message.lower() == 'exit':\n",
    "                print(\"üëã Ending chat session.\")\n",
    "                self.running = False\n",
    "                break\n",
    "                \n",
    "            elif user_message.lower() == 'clear':\n",
    "                self.messages = []\n",
    "                clear_output(wait=True)\n",
    "                print(f\"ü§ñ Chat reset with model: {self.model}\")\n",
    "                print(\"Type 'exit' to end the chat, 'clear' to reset the conversation, or 'change model:[name]' to switch models.\")\n",
    "                continue\n",
    "                \n",
    "            elif user_message.lower().startswith('change model:'):\n",
    "                new_model = user_message[len('change model:'):].strip()\n",
    "                if new_model:\n",
    "                    self.model = new_model\n",
    "                    print(f\"üîÑ Model changed to {self.model}\")\n",
    "                    continue\n",
    "                else:\n",
    "                    self.display_models()\n",
    "                    continue\n",
    "                    \n",
    "            elif user_message.lower() == 'models':\n",
    "                self.display_models()\n",
    "                continue\n",
    "                \n",
    "            # Skip empty messages\n",
    "            if not user_message.strip():\n",
    "                continue\n",
    "                \n",
    "            # Add user message to history\n",
    "            self.messages.append({'role': 'user', 'content': user_message})\n",
    "            \n",
    "            # Get AI response with streaming\n",
    "            print(\"\\nü§ñ AI: \", end='')\n",
    "            \n",
    "            try:\n",
    "                full_response = \"\"\n",
    "                stream = ollama.chat(\n",
    "                    model=self.model,\n",
    "                    messages=self.messages,\n",
    "                    stream=True,\n",
    "                )\n",
    "                \n",
    "                for chunk in stream:\n",
    "                    if 'message' in chunk and 'content' in chunk['message']:\n",
    "                        text_chunk = chunk['message']['content']\n",
    "                        full_response += text_chunk\n",
    "                        print(text_chunk, end='', flush=True)\n",
    "                \n",
    "                # Add AI response to history\n",
    "                self.messages.append({'role': 'assistant', 'content': full_response})\n",
    "                print()  # Add newline after response\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ö†Ô∏è Error: {str(e)}\")\n",
    "                \n",
    "                # If model not found, suggest listing models\n",
    "                if \"model not found\" in str(e).lower():\n",
    "                    print(f\"Model '{self.model}' not found. Type 'models' to see available models.\")\n",
    "                    \n",
    "                    # Reset to a likely available model\n",
    "                    default_models = [\"llama3.2:3b\", \"llama3.2\", \"llama3\", \"phi3\"]\n",
    "                    for model in default_models:\n",
    "                        try:\n",
    "                            # Try to pull model info to check if it exists\n",
    "                            ollama.show(model)\n",
    "                            self.model = model\n",
    "                            print(f\"Switched to model: {self.model}\")\n",
    "                            break\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "# Create and start the chat interface\n",
    "chat = SimpleOllamaChat()\n",
    "chat.start_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgAnno2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
