{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama 101 - A set of basic ollama functions from the ollama pypi docs\n",
    "All functions are referenced from the PyPi ollama package built by Jeffrey Morgan: https://pypi.org/project/ollama/\n",
    "\n",
    "The following notebook is a set of ollama tools and part of an ollama guide made by Leo Borcherding. \n",
    "This notebook is part of the following ollama tutorial https://github.com/Leoleojames1/ollamaStarterKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ollama in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (0.4.7)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from ollama) (2.10.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from ipywidgets) (8.32.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in c:\\users\\ada\\miniconda3\\envs\\imganno2\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "# Install Ollama\n",
    "!pip install ollama\n",
    "# Install ipywidgets\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama pull/run\n",
    "\n",
    "The following commands will allow you to pull any ollama model, feel free to explore ollama's vast set of models to choose from, here are some of my custom system prompts: https://ollama.com/borch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ollama run phi3\n",
    "\n",
    "&\n",
    "\n",
    "ollama pull phi3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama show & list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ollama show, shows the currently loaded model's modelfile metadata*\n",
    "ollama show --modelfile llama3.2:3b\n",
    "\n",
    "*ollama list shows the available models*\n",
    "ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": \"llama3.2:3b\",\n",
      "  \"modified_at\": \"2024-09-25 17:58:55.324918-05:00\",\n",
      "  \"family\": \"llama\",\n",
      "  \"parameter_size\": \"3.2B\",\n",
      "  \"quantization\": \"Q4_K_M\",\n",
      "  \"parameters\": [\n",
      "    \"stop                           \\\"<|start_header_id|>\\\"\",\n",
      "    \"stop                           \\\"<|end_header_id|>\\\"\",\n",
      "    \"stop                           \\\"<|eot_id|>\\\"\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "MODEL                                    FAMILY     SIZE       QUANT     \n",
      "---------------------------------------------------------------------------\n",
      "hf.co/unsloth/gemma-3-4b-it-GGUF:Q4_K_   gemma3     3.34 GB    unknown   \n",
      "hf.co/Borcherding/Llama-3.2-3B-Finetom   llama      2.02 GB    unknown   \n",
      "hf.co/Borcherding/OARC_Commander_v001_   llama      6.43 GB    unknown   \n",
      "hf.co/Borcherding/OARC_Commander_v001_   llama      3.42 GB    unknown   \n",
      "dolphin3:latest                          llama      4.92 GB    Q4_K_M    \n",
      "marco-o1:latest                          qwen2      4.68 GB    Q4_K_M    \n",
      "qwen2.5-coder:latest                     qwen2      4.68 GB    Q4_K_M    \n",
      "opencoder:latest                         llama      4.74 GB    Q4_K_M    \n",
      "unclemusclez/unsloth-llama3.2:latest     llama      1.92 GB    Q4_0      \n",
      "wojtek/beyonder:4x7b-v3-q4_k             llama      14.61 GB   Q4_K_M    \n",
      "llama2:latest                            llama      3.83 GB    Q4_0      \n",
      "llama3.2:1b-instruct-q2_K                llama      0.58 GB    Q2_K      \n",
      "llama3.2:1b                              llama      1.32 GB    Q8_0      \n",
      "llama3.2:3b                              llama      2.02 GB    Q4_K_M    \n",
      "eas/nous-genstruct:7b-f16                llama      14.48 GB   F16       \n",
      "unclemusclez/smollm-135m-instruct-devi   llama      0.27 GB    F16       \n",
      "borch/Meta-Llama-3.1-8B-Instruct-bnb-4   llama      15.08 GB   Q8_0      \n",
      "CognitiveComputations/dolphin-llama3.1   llama      4.66 GB    Q4_0      \n",
      "stablelm2:1.6b                           stablelm   0.98 GB    Q4_0      \n",
      "smollm:1.7b                              llama      0.99 GB    Q4_0      \n",
      "smollm:360m                              llama      0.23 GB    Q4_0      \n",
      "smollm:135m                              llama      0.09 GB    Q4_0      \n",
      "mannix/llama3.1-8b-abliterated:latest    llama      4.66 GB    Q4_0      \n",
      "sebdg/emotional_llama:latest             llama      4.92 GB    Q4_K_M    \n",
      "mathstral:latest                         llama      4.11 GB    Q4_0      \n",
      "llama3.1:8b                              llama      4.66 GB    Q4_0      \n",
      "qnguyen3/nanollava:latest                qwen2      2.11 GB    F16       \n",
      "CognitiveComputations/dolphin-2.9.2-qw   qwen2      3.02 GB    Q2_K      \n",
      "qwen2:latest                             qwen2      4.43 GB    Q4_0      \n",
      "borch/emotional_llama_speed_chat:lates   llama      4.92 GB    Q4_K_M    \n",
      "nomic-embed-text:latest                  nomic-be   0.27 GB    F16       \n",
      "borch/phi3_speed_chat:latest             llama      2.32 GB    Q4_K_M    \n",
      "phi3:latest                              llama      2.32 GB    Q4_K_M    \n",
      "borch/gpt2-chatbot-q8_0:latest           gpt2       1.75 GB    Q8_0      \n",
      "sunapi386/llama-3-lexi-uncensored:8b     llama      4.92 GB    Q4_K_M    \n",
      "mistral:latest                           llama      4.11 GB    Q4_0      \n",
      "llava:latest                             llama      4.73 GB    Q4_0      \n",
      "llava-phi3:latest                        llama      2.93 GB    Q4_K_M    \n",
      "llama3:latest                            llama      4.66 GB    Q4_0      \n",
      "dolphin-phi:latest                       phi2       1.60 GB    Q4_0      \n",
      "dolphin-mistral:latest                   llama      4.11 GB    Q4_0      \n",
      "dolphin-llama3:latest                    llama      4.66 GB    Q4_0      \n",
      "borch/llama3_speed_chat_2:latest         llama      4.66 GB    Q4_0      \n",
      "borch/llava-llama-3-8b-Q3_K_M:latest     llama      4.02 GB    Q3_K_M    \n",
      "borch/llava-phi-3-mini-int4:latest       llama      2.32 GB    Q4_K_M    \n",
      "r2-d2:latest                             llama      4.66 GB    Q4_0      \n",
      "RicknMorty:latest                        llama      4.66 GB    Q4_0      \n",
      "Dolphin_Eminem_Rap_God:latest            llama      4.11 GB    Q4_0      \n",
      "borch_llama3po:latest                    llama      4.66 GB    Q4_0      \n",
      "borch_llama3_speed_chat_shrek:latest     llama      4.66 GB    Q4_0      \n",
      "borch_llama3_speed_chat:latest           llama      4.66 GB    Q4_0      \n",
      "Jesus:latest                             llama      4.66 GB    Q4_0      \n",
      "C3PO:latest                              llama      4.66 GB    Q4_0      \n"
     ]
    }
   ],
   "source": [
    "# Import the Ollama library\n",
    "import ollama\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Show information about a specific model\n",
    "model_info = ollama.show('llama3.2:3b')\n",
    "\n",
    "# Convert to a dictionary for easier formatting\n",
    "model_dict = {\n",
    "    \"model\": \"llama3.2:3b\",\n",
    "    \"modified_at\": str(model_info.modified_at),\n",
    "    \"family\": model_info.details.family,\n",
    "    \"parameter_size\": model_info.details.parameter_size,\n",
    "    \"quantization\": model_info.details.quantization_level,\n",
    "    \"parameters\": model_info.parameters.split('\\n')\n",
    "}\n",
    "\n",
    "# Pretty print the model information\n",
    "print(json.dumps(model_dict, indent=2))\n",
    "\n",
    "# List all available models\n",
    "models = ollama.list()\n",
    "\n",
    "# Print a simplified table of models\n",
    "print(\"\\n{:<40} {:<10} {:<10} {:<10}\".format(\"MODEL\", \"FAMILY\", \"SIZE\", \"QUANT\"))\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for model in models.models:\n",
    "    size_gb = f\"{model.size / 1_000_000_000:.2f} GB\"\n",
    "    print(\"{:<40} {:<10} {:<10} {:<10}\".format(\n",
    "        model.model[:38], \n",
    "        model.details.family[:8], \n",
    "        size_gb, \n",
    "        model.details.quantization_level[:8]\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ChatResponse method can be used for direct back and forth messaging with your ollama model, but will not provide access to prompt streaming. For Streaming follow along to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\" \n",
      "\n",
      "I was trained on a combined text dataset consisting of books and articles from the internet. This training allows me to generate human-like responses to a wide range of questions and topics.\n",
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\" \n",
      "\n",
      "I was trained on a combined text dataset consisting of books and articles from the internet. This training allows me to generate human-like responses to a wide range of questions and topics.\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='llama3.2:3b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Who are you? And what were you trained on?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a nice interactive chat interface for your Ollama models that works directly in the notebook. This interface includes:\n",
    "\n",
    "1. A dropdown to select the ollama model\n",
    "2. A text input for messages\n",
    "3. A scrollable chat history display\n",
    "4. A reset button to start fresh conversations\n",
    "5. Streaming responses that appear character by character\n",
    "\n",
    "To use it, simply run this cell and you'll get a fully functional chat interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Chat initialized with model: llama3.2:3b\n",
      "Type 'exit' to end the chat, 'clear' to reset the conversation, or 'change model:[name]' to switch models.\n",
      "\n",
      "ü§ñ AI: Hello! How can I assist you today?\n",
      "\n",
      "ü§ñ AI: I'm just a language model, so I don't have emotions or feelings like humans do. But I'm functioning properly and ready to help with any questions or topics you'd like to discuss!\n",
      "\n",
      "How about you? How's your day going so far?\n",
      "\n",
      "ü§ñ AI: Power series solutions for differential equations are a fundamental topic in mathematics and physics. Here's an overview of the method, along with some LaTeX code examples.\n",
      "\n",
      "**Method**\n",
      "\n",
      "Assume you have a second-order linear homogeneous differential equation of the form:\n",
      "\n",
      "$$y'' + p(x)y' + q(x)y = 0$$\n",
      "\n",
      "where $p(x)$ and $q(x)$ are continuous functions on an interval $[a, b]$. We want to find a power series solution of the form:\n",
      "\n",
      "$$y(x) = \\sum_{n=0}^{\\infty} a_n x^n$$\n",
      "\n",
      "Substituting this expression into the differential equation, we get:\n",
      "\n",
      "$$(\\sum_{n=2}^{\\infty} n(n-1)a_n x^{n-2}) + p(x)(\\sum_{n=1}^{\\infty} na_n x^{n-1}) + q(x)(\\sum_{n=0}^{\\infty} a_n x^n) = 0$$\n",
      "\n",
      "Equate coefficients of like powers of $x$ to zero:\n",
      "\n",
      "For $n = 0$, we get the indicial equation:\n",
      "\n",
      "$$a_0 + a_1 p(a) + a_0 q(a) = 0$$\n",
      "\n",
      "For $n \\geq 1$, we get a recurrence relation for the coefficients $a_n$:\n",
      "\n",
      "$$a_{n+2} = \\frac{-(p(x)a_{n+1} + q(x)a_n)}{(n+2)(n+1)}$$\n",
      "\n",
      "**LaTeX Code**\n",
      "\n",
      "Here's an example of how to typeset this in LaTeX:\n",
      "```latex\n",
      "\\documentclass{article}\n",
      "\\usepackage{amsmath}\n",
      "\n",
      "\\begin{document}\n",
      "\n",
      "\\section{Power Series Solutions for Differential Equations}\n",
      "\n",
      "Assume we have a second-order linear homogeneous differential equation of the form:\n",
      "\n",
      "$$y'' + p(x)y' + q(x)y = 0$$\n",
      "\n",
      "We want to find a power series solution of the form:\n",
      "\n",
      "$$y(x) = \\sum_{n=0}^{\\infty} a_n x^n$$\n",
      "\n",
      "Substituting this expression into the differential equation, we get:\n",
      "\n",
      "$$(\\sum_{n=2}^{\\infty} n(n-1)a_n x^{n-2}) + p(x)(\\sum_{n=1}^{\\infty} na_n x^{n-1}) + q(x)(\\sum_{n=0}^{\\infty} a_n x^n) = 0$$\n",
      "\n",
      "Equate coefficients of like powers of $x$ to zero:\n",
      "\n",
      "For $n = 0$, we get the indicial equation:\n",
      "\n",
      "$$a_0 + a_1 p(a) + a_0 q(a) = 0$$\n",
      "\n",
      "For $n \\geq 1$, we get a recurrence relation for the coefficients $a_n$:\n",
      "\n",
      "$$a_{n+2} = \\frac{-(p(x)a_{n+1} + q(x)a_n)}{(n+2)(n+1)}$$\n",
      "\n",
      "\\end{document}\n",
      "```\n",
      "This code produces the following output:\n",
      "\n",
      "Assume we have a second-order linear homogeneous differential equation of the form:\n",
      "y'' + p(x)y' + q(x)y = 0\n",
      "We want to find a power series solution of the form:\n",
      "y(x) = \\sum_{n=0}^{\\infty} a_n x^n\n",
      "\n",
      "Substituting this expression into the differential equation, we get:\n",
      "\n",
      "$(\\sum_{n=2}^{\\infty} n(n-1)a_n x^{n-2}) + p(x)(\\sum_{n=1}^{\\infty} na_n x^{n-1}) + q(x)(\\sum_{n=0}^{\\infty} a_n x^n) = 0$\n",
      "\n",
      "Equate coefficients of like powers of $x$ to zero:\n",
      "\n",
      "For $n = 0$, we get the indicial equation:\n",
      "a_0 + a_1 p(a) + a_0 q(a) = 0\n",
      "\n",
      "For $n \\geq 1$, we get a recurrence relation for the coefficients $a_n$:\n",
      "$a_{n+2} = \\frac{-(p(x)a_{n+1} + q(x)a_n)}{(n+2)(n+1)}$\n",
      "\n",
      "Note that this is just a brief overview of the method, and there are many variations and extensions to power series solutions for differential equations.\n",
      "\n",
      "ü§ñ AI: It looks like you're testing the LaTeX code!\n",
      "\n",
      "If you want to try something else or ask about a specific aspect of power series solutions for differential equations, feel free to let me know!\n",
      "\n",
      "ü§ñ AI: Here is a simple dragon made out of pixels using emojis:\n",
      "\n",
      "\n",
      " /_/\\\n",
      "( o.o )\n",
      " > ^ <\n",
      "__/\\___\n",
      "\n",
      "üêâ\n",
      "\n",
      "I hope you like it! üòä\n",
      "\n",
      "ü§ñ AI: Here's an even bigger dragon, made up of even more pixels (or emojis, in this case):\n",
      "\n",
      "\n",
      " _______________________\n",
      "|                       |\n",
      "|  /_\\      \\       __  |\n",
      "| ( o.o )  > ^ <    /  \\\n",
      "|_____________________|  |_____|\n",
      "|                          |  | \n",
      "|  _____  ___ ___   _____  |  | \n",
      "| /     \\/   \\/     \\\\/     |  | \n",
      "| |  O   | O   |    O   |  | | \n",
      "| |__|   |__|   |    |__| | |\n",
      "|_________________________| |_____|\n",
      "|                          |_______|\n",
      "\n",
      "ü§ñ AI: Here's a simple representation of the Earth, made out of emojis:\n",
      "\n",
      "\n",
      "üåé\n",
      "  üåä (ocean)\n",
      "    üèîÔ∏è  üå≥  üå¥ (land with trees and palm fronds)\n",
      "      ‚òÄÔ∏è  ‚òÅÔ∏è  üåÄ (sun, clouds, and air currents)\n",
      "\n",
      "Or, if you want a more detailed representation of the Earth's layers:\n",
      "\n",
      "\n",
      "üåé\n",
      "  üî¥  üíõ  üîµ  üî∂ (atmosphere, hydrosphere, lithosphere, and mantle)\n",
      "    üåä  üèîÔ∏è  üå≥  üå¥  üê†  üê≥  ü¶ñ (ocean, land with trees and animals)\n",
      "      ‚òÄÔ∏è  ‚òÅÔ∏è  üåÄ  üîã (sun, clouds, air currents, and lightning)\n",
      "\n",
      "I hope you find this representation of the Earth visually appealing! üòä\n",
      "\n",
      "ü§ñ AI: Here's a simple representation of the Earth as a huge sphere made out of emojis:\n",
      "\n",
      "\n",
      "üåéüåéüåéüåéüåé\n",
      " üåé       üåé\n",
      " üåé     üåé  üåé\n",
      " üåé   üåé    üåé üåé\n",
      " üåé üåé      üåé     üåé\n",
      " üåé üåé        üåé        üåé\n",
      "\n",
      "Or, if you want a more symmetrical representation:\n",
      "\n",
      "\n",
      "üåéüåéüåéüåéüåéüåéüåéüåé\n",
      "  üåé       üåé       üåé\n",
      "   üåé     üåé         üåé\n",
      "    üåé   üåé           üåé \n",
      "     üåé üåé             üåé \n",
      "      üåé  üåé               üåé\n",
      "       üåé                 üåé\n",
      "\n",
      "I hope this represents the Earth as a huge sphere in a way that you like! üòä\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class SimpleOllamaChat:\n",
    "    def __init__(self, model=\"llama3.2:3b\"):\n",
    "        \"\"\"Initialize the chat interface with a specified model.\"\"\"\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "        self.running = True\n",
    "        print(f\"ü§ñ Chat initialized with model: {self.model}\")\n",
    "        print(\"Type 'exit' to end the chat, 'clear' to reset the conversation, or 'change model:[name]' to switch models.\")\n",
    "        \n",
    "    def display_models(self):\n",
    "        \"\"\"Display available models in a clean format.\"\"\"\n",
    "        try:\n",
    "            models_list = ollama.list()\n",
    "            print(\"\\nAvailable models:\")\n",
    "            print(\"-\" * 60)\n",
    "            for model in models_list.models[:10]:  # Show only first 10 models to avoid clutter\n",
    "                print(f\"‚Ä¢ {model.model}\")\n",
    "            if len(models_list.models) > 10:\n",
    "                print(f\"... and {len(models_list.models) - 10} more models\")\n",
    "            print(\"-\" * 60)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error fetching models: {e}\")\n",
    "    \n",
    "    def start_chat(self):\n",
    "        \"\"\"Start the interactive chat loop.\"\"\"\n",
    "        while self.running:\n",
    "            # Get user input\n",
    "            user_message = input(\"\\nüë§ You: \")\n",
    "            \n",
    "            # Process commands\n",
    "            if user_message.lower() == 'exit':\n",
    "                print(\"üëã Ending chat session.\")\n",
    "                self.running = False\n",
    "                break\n",
    "                \n",
    "            elif user_message.lower() == 'clear':\n",
    "                self.messages = []\n",
    "                clear_output(wait=True)\n",
    "                print(f\"ü§ñ Chat reset with model: {self.model}\")\n",
    "                print(\"Type 'exit' to end the chat, 'clear' to reset the conversation, or 'change model:[name]' to switch models.\")\n",
    "                continue\n",
    "                \n",
    "            elif user_message.lower().startswith('change model:'):\n",
    "                new_model = user_message[len('change model:'):].strip()\n",
    "                if new_model:\n",
    "                    self.model = new_model\n",
    "                    print(f\"üîÑ Model changed to {self.model}\")\n",
    "                    continue\n",
    "                else:\n",
    "                    self.display_models()\n",
    "                    continue\n",
    "                    \n",
    "            elif user_message.lower() == 'models':\n",
    "                self.display_models()\n",
    "                continue\n",
    "                \n",
    "            # Skip empty messages\n",
    "            if not user_message.strip():\n",
    "                continue\n",
    "                \n",
    "            # Add user message to history\n",
    "            self.messages.append({'role': 'user', 'content': user_message})\n",
    "            \n",
    "            # Get AI response with streaming\n",
    "            print(\"\\nü§ñ AI: \", end='')\n",
    "            \n",
    "            try:\n",
    "                full_response = \"\"\n",
    "                stream = ollama.chat(\n",
    "                    model=self.model,\n",
    "                    messages=self.messages,\n",
    "                    stream=True,\n",
    "                )\n",
    "                \n",
    "                for chunk in stream:\n",
    "                    if 'message' in chunk and 'content' in chunk['message']:\n",
    "                        text_chunk = chunk['message']['content']\n",
    "                        full_response += text_chunk\n",
    "                        print(text_chunk, end='', flush=True)\n",
    "                \n",
    "                # Add AI response to history\n",
    "                self.messages.append({'role': 'assistant', 'content': full_response})\n",
    "                print()  # Add newline after response\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ö†Ô∏è Error: {str(e)}\")\n",
    "                \n",
    "                # If model not found, suggest listing models\n",
    "                if \"model not found\" in str(e).lower():\n",
    "                    print(f\"Model '{self.model}' not found. Type 'models' to see available models.\")\n",
    "                    \n",
    "                    # Reset to a likely available model\n",
    "                    default_models = [\"llama3.2:3b\", \"llama3.2\", \"llama3\", \"phi3\"]\n",
    "                    for model in default_models:\n",
    "                        try:\n",
    "                            # Try to pull model info to check if it exists\n",
    "                            ollama.show(model)\n",
    "                            self.model = model\n",
    "                            print(f\"Switched to model: {self.model}\")\n",
    "                            break\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "# Create and start the chat interface\n",
    "chat = SimpleOllamaChat()\n",
    "chat.start_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgAnno2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
